{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from dumbrain.ml.kaggle.download import download\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = download( 'titanic' )\n",
    "test_file, train_file, example_output = list( map( lambda file: 'data/' + file, data_files ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data = pd.read_csv( train_file )\n",
    "all_test_data = pd.read_csv( test_file )\n",
    "all_example_data = pd.read_csv( example_output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class DataCleaner( metaclass=abc.ABCMeta ):\n",
    "    def __init__( self ):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def clean( self, data ):\n",
    "        pass\n",
    "\n",
    "class ColumnCleaner( DataCleaner ):\n",
    "    def __init__( self, column_name ):\n",
    "        super( ColumnCleaner, self ).__init__()\n",
    "        self.column_name = column_name\n",
    "\n",
    "class DummyColumnCleaner( ColumnCleaner ):\n",
    "    def __init__( self, column_name, all_values ):\n",
    "        super( DummyColumnCleaner, self ).__init__( column_name )\n",
    "        self.all_values = all_values\n",
    "\n",
    "    def clean( self, data ):\n",
    "        data = data.copy()\n",
    "        dummy_cols = pd.get_dummies( data[ self.column_name ], prefix='' )\n",
    "        dummy_cols = dummy_cols.add_suffix( '_' + self.column_name )\n",
    "        data = data.join( dummy_cols, rsuffix=self.column_name )\n",
    "        data = data.drop( self.column_name, axis=1 )\n",
    "        return data\n",
    "\n",
    "class RemoveColumnCleaner( ColumnCleaner ):\n",
    "    def clean( self, data ):\n",
    "        if self.column_name in data.columns:\n",
    "            return data.drop( self.column_name, axis=1 )\n",
    "        return data\n",
    "\n",
    "class FilterDataCleaner( DataCleaner ):\n",
    "    def __init__( self, filter_func ):\n",
    "        super( FilterClenaer, self ).__init__( column_name )\n",
    "        self.filter_func = filter_func\n",
    "\n",
    "    def clean( self, data ):\n",
    "        return data[ self.filter_func( data ) ]\n",
    "\n",
    "class FillNaNDataCleaner( DataCleaner ):\n",
    "    def __init__( self, new_value ):\n",
    "        super( FillNaNDataCleaner, self ).__init__()\n",
    "        self.new_value = new_value\n",
    "\n",
    "    def clean( self, data ):\n",
    "        return data.fillna( self.new_value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameSentimentColumnCleaner( ColumnCleaner ):\n",
    "    def __init__( self, column_name, train_data, score_column_name, omit=[], minlen=2 ):\n",
    "        super( NameSentimentColumnCleaner, self ).__init__( column_name )\n",
    "        self.train_data = train_data\n",
    "        self.score_column_name = score_column_name\n",
    "        self.omit = omit\n",
    "        self.minlen = minlen\n",
    "        self.generateSentiments()\n",
    "\n",
    "    def tokenize( self, string ):\n",
    "        strings_to_remove = [ '.', ',', '(', ')', \"'\", '\"' ] + self.omit\n",
    "        for string_to_remove in strings_to_remove:\n",
    "            string = string.replace( string_to_remove, '' )\n",
    "        string = string.lower()\n",
    "        tokens = string.split( ' ' )\n",
    "        filtered_tokens = []\n",
    "#         for token in tokens:\n",
    "#             if token\n",
    "        tokens = filter( lambda item: len( item ) >= self.minlen, tokens )\n",
    "        return list( tokens )\n",
    "\n",
    "    def generateSentiments( self ):\n",
    "        tokens = []\n",
    "        scores = []\n",
    "        for i, row in self.train_data.iterrows():\n",
    "            for token in self.tokenize( row[ self.column_name ] ):\n",
    "                tokens.append( token )\n",
    "                scores.append( row[ self.score_column_name ] )\n",
    "        df = pd.DataFrame( { 'tokens': tokens, 'scores': scores } )\n",
    "        grouped = df.groupby( 'tokens' ).mean()\n",
    "        grouped = grouped[ df.groupby( 'tokens' ).count()[ 'scores' ] > 5 ]\n",
    "        self.sentiments = grouped\n",
    "\n",
    "    def clean( self, data ):\n",
    "        data = data.copy()\n",
    "        \n",
    "        scores = []\n",
    "        for i, row in data.iterrows():\n",
    "            tokens = self.tokenize( row[ self.column_name ] )\n",
    "            score = self.sentiments.iloc[ self.sentiments.index.isin( tokens ) ].mean()[ 'scores' ]\n",
    "            scores.append( score )\n",
    "    \n",
    "        data[ self.column_name + '_score' ] = scores\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_uncleaned = all_train_data.sample( frac=0.8, random_state=4111 )\n",
    "validate_data_uncleaned = all_train_data.drop( train_data.index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaners = [\n",
    "    RemoveColumnCleaner( 'PassengerId' ),\n",
    "#     NameSentimentColumnCleaner( 'Name', train_data_uncleaned, 'Survived', omit=[ 'mrs', 'miss', 'mr' ] ),\n",
    "    RemoveColumnCleaner( 'Name' ),  \n",
    "    DummyColumnCleaner( 'Sex', [ 'male', 'female' ] ),\n",
    "    RemoveColumnCleaner( 'Sex' ),\n",
    "    DummyColumnCleaner( 'Pclass', [ 1, 2, 3 ] ),\n",
    "    RemoveColumnCleaner( 'Ticket' ),                     # Todo: Use this data\n",
    "    RemoveColumnCleaner( 'Cabin' ),                      # Todo: Use this data\n",
    "    DummyColumnCleaner( 'Embarked', [ 'S', 'C', 'Q' ] ),\n",
    "    FillNaNDataCleaner( 0 )\n",
    "]\n",
    "\n",
    "def cleanData( _cleaners, _data ):\n",
    "    for cleaner in _cleaners:\n",
    "        _data = cleaner.clean( _data )\n",
    "    return _data\n",
    "\n",
    "train_data = cleanData( cleaners, train_data_uncleaned )\n",
    "validate_data = cleanData( cleaners, validate_data_uncleaned )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Survived', 'Age', 'SibSp', 'Parch', 'Fare', '_female_Sex', '_male_Sex',\n",
       "       '_1_Pclass', '_2_Pclass', '_3_Pclass', '_C_Embarked', '_Q_Embarked',\n",
       "       '_S_Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mike/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit( train_data.drop( 'Survived', axis=1 ), train_data[ 'Survived' ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8258426966292135"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score( validate_data.drop( 'Survived', axis=1 ), validate_data[ 'Survived' ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_cleaned = cleanData( cleaners, all_test_data )\n",
    "predicted = model.predict( test_data_cleaned )\n",
    "\n",
    "output = pd.DataFrame( { 'PassengerId': all_test_data[ 'PassengerId' ], 'Survived': predicted } )\n",
    "output = output.set_index( 'PassengerId' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv( 'data/results.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
